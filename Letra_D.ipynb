{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPdFxE74o1RA98XeR8bUwYj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Otimização de Rede Neural com Gradiente Descendente ($b_3$)\n","\n","utilizaremos o **Gradiente Descendente** para refinar o ajuste final de uma rede neural. Com o objetivo de encontrar o valor ideal para o viés da camada de saída (**$b_3$**), mantendo os outros pesos fixos.\n","\n","A estrutura matemática da rede (Forward Pass) é definida por:\n","\n","$$\n","h_1 = \\ln(1 + e^{(x \\cdot w_1 + b_1)})\n","$$\n","\n","$$\n","h_2 = \\ln(1 + e^{(x \\cdot w_2 + b_2)})\n","$$\n","\n","$$\n","\\hat{y} = (h_1 \\cdot w_3 + h_2 \\cdot w_4) + \\mathbf{b_3}\n","$$\n","\n","**Onde:**\n","* **$x$**: Dado de entrada.\n","* **$h_1, h_2$**: Neurônios da camada oculta (ativados pela função **Softplus**).\n","* **$w_1...w_4$**: Pesos da rede (que manteremos congelados/fixos).\n","* **$b_3$**: **Viés Final** (o parâmetro que vamos otimizar dinamicamente).\n","* **$\\hat{y}$**: A previsão final da rede.\n","\n","O algoritmo calculará o gradiente (a direção do erro) considerando **todo o conjunto de dados** a cada iteração, ajustando $b_3$ para minimizar a diferença entre a curva prevista ($\\hat{y}$) e os dados reais."],"metadata":{"id":"ceFke6PfkERG"}},{"cell_type":"markdown","source":["##Bibliotecas que usaremos\n","\n","\n"],"metadata":{"id":"06gBon-mSFTF"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"3vwQzpidZKJf","executionInfo":{"status":"ok","timestamp":1765416399691,"user_tz":180,"elapsed":111,"user":{"displayName":"artz","userId":"02564237698216966018"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation\n","from IPython.display import HTML"]},{"cell_type":"markdown","source":["## Definição das variaveis com os dados que usaremos:"],"metadata":{"id":"X1Kb3_Dp8AeI"}},{"cell_type":"code","source":["w1, w2, w3, w4 = 3.34, -3.53, -1.22, -2.3\n","b1, b2 = -1.43, 0.57\n","b3 = 0\n","values_of_b3 = []\n","list_current_gradient = []\n","list_sum_of_squared_residuals = []\n","\n","x_data = np.array([0, 0.5, 1])\n","y_data = np.array([0, 1, 0])\n","max_iterations = 1000\n","precision = 0.0001\n","learning_rate = 0.01"],"metadata":{"id":"Xg4zQSW2qvVX","executionInfo":{"status":"ok","timestamp":1765416399698,"user_tz":180,"elapsed":3,"user":{"displayName":"artz","userId":"02564237698216966018"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Definição das funções\n","Para facilitar a leitura, vamos associar as variáveis do código aos símbolos matemáticos:\n","\n","* **$x$**: `input` (Dados de entrada)\n","* **$w_1, w_2$**: Pesos da primeira camada\n","* **$b_1, b_2$**: Vieses da primeira camada\n","* **$h_1, h_2$**: `y_curve1`, `y_curve2` (Saída dos neurônios da camada oculta, após ativação)\n","* **$w_3, w_4$**: Pesos da camada de saída\n","* **$b_3$**: Viés final\n","* **$\\hat{y}$**: `y_curve` / `predictions` (Previsão final)\n"],"metadata":{"id":"SY7kx0Z2ny_A"}},{"cell_type":"markdown","source":["\n","## 1. Equação da Rede Neural (Forward Pass)\n","Correspondente à função `neural_network`.\n","\n","* Cálcula a ativação dos neurônios ocultos usando a função **Softplus** ($\\ln(1+e^z)$)\n","* Combina os resultados linearmente.\n","\n","$$\n","h_1 = \\ln(1 + e^{(x \\cdot w_1 + b_1)})\n","$$\n","\n","$$\n","h_2 = \\ln(1 + e^{(x \\cdot w_2 + b_2)})\n","$$\n","\n","**A saída final é:**\n","\n","$$\n","\\hat{y} = (h_1 \\cdot w_3 + h_2 \\cdot w_4) + b_3\n","$$\n"],"metadata":{"id":"Ebd-ActgmW6j"}},{"cell_type":"code","source":["def neural_network(input, b3):\n","  y_curve1 = np.log(1 + np.exp(input*w1+b1))\n","  y_curve2 = np.log(1 + np.exp(input*w2+b2))\n","  y_curve = (y_curve1*w3+y_curve2*w4) +b3\n","  return y_curve"],"metadata":{"id":"zArGOyWBMVk1","executionInfo":{"status":"ok","timestamp":1765416399824,"user_tz":180,"elapsed":83,"user":{"displayName":"artz","userId":"02564237698216966018"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["\n","## 2. Derivada da Função de Custo\n","Correspondente à função `calculate_derivative`.\n","\n","Derivada da Soma dos Quadrados dos Resíduos ($SSR$) em relação ao parâmetro $b_3$.\n","\n","$$\n","\\frac{\\partial SSR}{\\partial b_3} = \\sum_{i=1}^{n} -2 (y_i - \\hat{y}_i)\n","$$"],"metadata":{"id":"wvat1ZGGn3uS"}},{"cell_type":"code","source":["def calculate_derivative(b3):\n","  predictions = neural_network(x_data, b3)\n","  residuals = y_data - predictions\n","  return np.sum(-2 * residuals)"],"metadata":{"id":"KigXqPZbn5gp","executionInfo":{"status":"ok","timestamp":1765416399828,"user_tz":180,"elapsed":30,"user":{"displayName":"artz","userId":"02564237698216966018"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Loop de Treinamento: Otimizando $b_3$\n","\n","A rede neural aprende o melhor valor para o viés final ($b_3$) repetindo o seguinte ciclo:\n","\n","1.  **Forward Pass:** A rede faz uma previsão com o valor atual de $b_3$.\n","2.  **Cálculo do Erro:** Medimos a distância entre a previsão e o valor real (SSR).\n","3.  **Backward Pass:** Calculamos o gradiente (`calculate_derivative`) para descobrir a direção do ajuste.\n","4.  **Atualização:** Modificamos $b_3$ usando a taxa de aprendizado.\n","5. **Verifica a Precisão e se os passos de atualização para o $b_3$forem pequenos demais**\n","\n","O processo se repete até que o gradiente seja próximo de zero (convergência) ou o maximo de iterações for atingido.\n","\n","Também armazenamos o histórico dos valores de $b_3$ para a visualização da convergência do algoritmo em gráficos posteriores."],"metadata":{"id":"RH8-sTYNo1xu"}},{"cell_type":"code","source":["for i in range(max_iterations):\n","    values_of_b3.append(b3)\n","\n","    predictions = neural_network(x_data, b3)\n","    sum_of_squared_residuals = np.sum((y_data - predictions)**2)\n","    list_sum_of_squared_residuals.append(sum_of_squared_residuals)\n","\n","    current_gradient = calculate_derivative(b3)\n","    list_current_gradient.append(current_gradient)\n","\n","    print(f\"\"\"Step Size: {(learning_rate * current_gradient):.4f}\n","Old b3: {b3:.4f}\n","New b3: {(b3 - learning_rate * current_gradient):.4f}\\n\"\"\",\n","              flush=True)\n","\n","    b3 = b3 - learning_rate * current_gradient\n","    if np.abs(current_gradient*learning_rate) < precision:\n","        break\n","\n","print(f\"b3 Final: {b3:.4f}\")\n","print(f\"gradiente Final: {current_gradient:.4f}\")\n","print(f\"Total de Iterações: {len(values_of_b3)}\")"],"metadata":{"id":"seTMCCVsNdY-","executionInfo":{"status":"ok","timestamp":1765416399993,"user_tz":180,"elapsed":170,"user":{"displayName":"artz","userId":"02564237698216966018"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bcf4c9a8-5bf7-4483-f853-1fe4ab6a73e5","collapsed":true},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Step Size: -0.1566\n","Old b3: 0.0000\n","New b3: 0.1566\n","\n","Step Size: -0.1472\n","Old b3: 0.1566\n","New b3: 0.3037\n","\n","Step Size: -0.1383\n","Old b3: 0.3037\n","New b3: 0.4420\n","\n","Step Size: -0.1300\n","Old b3: 0.4420\n","New b3: 0.5721\n","\n","Step Size: -0.1222\n","Old b3: 0.5721\n","New b3: 0.6943\n","\n","Step Size: -0.1149\n","Old b3: 0.6943\n","New b3: 0.8092\n","\n","Step Size: -0.1080\n","Old b3: 0.8092\n","New b3: 0.9172\n","\n","Step Size: -0.1015\n","Old b3: 0.9172\n","New b3: 1.0187\n","\n","Step Size: -0.0954\n","Old b3: 1.0187\n","New b3: 1.1141\n","\n","Step Size: -0.0897\n","Old b3: 1.1141\n","New b3: 1.2038\n","\n","Step Size: -0.0843\n","Old b3: 1.2038\n","New b3: 1.2882\n","\n","Step Size: -0.0793\n","Old b3: 1.2882\n","New b3: 1.3674\n","\n","Step Size: -0.0745\n","Old b3: 1.3674\n","New b3: 1.4419\n","\n","Step Size: -0.0700\n","Old b3: 1.4419\n","New b3: 1.5120\n","\n","Step Size: -0.0658\n","Old b3: 1.5120\n","New b3: 1.5778\n","\n","Step Size: -0.0619\n","Old b3: 1.5778\n","New b3: 1.6397\n","\n","Step Size: -0.0582\n","Old b3: 1.6397\n","New b3: 1.6978\n","\n","Step Size: -0.0547\n","Old b3: 1.6978\n","New b3: 1.7525\n","\n","Step Size: -0.0514\n","Old b3: 1.7525\n","New b3: 1.8039\n","\n","Step Size: -0.0483\n","Old b3: 1.8039\n","New b3: 1.8522\n","\n","Step Size: -0.0454\n","Old b3: 1.8522\n","New b3: 1.8977\n","\n","Step Size: -0.0427\n","Old b3: 1.8977\n","New b3: 1.9404\n","\n","Step Size: -0.0401\n","Old b3: 1.9404\n","New b3: 1.9805\n","\n","Step Size: -0.0377\n","Old b3: 1.9805\n","New b3: 2.0182\n","\n","Step Size: -0.0355\n","Old b3: 2.0182\n","New b3: 2.0537\n","\n","Step Size: -0.0333\n","Old b3: 2.0537\n","New b3: 2.0870\n","\n","Step Size: -0.0313\n","Old b3: 2.0870\n","New b3: 2.1183\n","\n","Step Size: -0.0295\n","Old b3: 2.1183\n","New b3: 2.1478\n","\n","Step Size: -0.0277\n","Old b3: 2.1478\n","New b3: 2.1755\n","\n","Step Size: -0.0260\n","Old b3: 2.1755\n","New b3: 2.2015\n","\n","Step Size: -0.0245\n","Old b3: 2.2015\n","New b3: 2.2259\n","\n","Step Size: -0.0230\n","Old b3: 2.2259\n","New b3: 2.2489\n","\n","Step Size: -0.0216\n","Old b3: 2.2489\n","New b3: 2.2706\n","\n","Step Size: -0.0203\n","Old b3: 2.2706\n","New b3: 2.2909\n","\n","Step Size: -0.0191\n","Old b3: 2.2909\n","New b3: 2.3100\n","\n","Step Size: -0.0180\n","Old b3: 2.3100\n","New b3: 2.3279\n","\n","Step Size: -0.0169\n","Old b3: 2.3279\n","New b3: 2.3448\n","\n","Step Size: -0.0159\n","Old b3: 2.3448\n","New b3: 2.3607\n","\n","Step Size: -0.0149\n","Old b3: 2.3607\n","New b3: 2.3756\n","\n","Step Size: -0.0140\n","Old b3: 2.3756\n","New b3: 2.3896\n","\n","Step Size: -0.0132\n","Old b3: 2.3896\n","New b3: 2.4028\n","\n","Step Size: -0.0124\n","Old b3: 2.4028\n","New b3: 2.4152\n","\n","Step Size: -0.0116\n","Old b3: 2.4152\n","New b3: 2.4268\n","\n","Step Size: -0.0109\n","Old b3: 2.4268\n","New b3: 2.4377\n","\n","Step Size: -0.0103\n","Old b3: 2.4377\n","New b3: 2.4480\n","\n","Step Size: -0.0097\n","Old b3: 2.4480\n","New b3: 2.4577\n","\n","Step Size: -0.0091\n","Old b3: 2.4577\n","New b3: 2.4668\n","\n","Step Size: -0.0085\n","Old b3: 2.4668\n","New b3: 2.4753\n","\n","Step Size: -0.0080\n","Old b3: 2.4753\n","New b3: 2.4834\n","\n","Step Size: -0.0075\n","Old b3: 2.4834\n","New b3: 2.4909\n","\n","Step Size: -0.0071\n","Old b3: 2.4909\n","New b3: 2.4980\n","\n","Step Size: -0.0067\n","Old b3: 2.4980\n","New b3: 2.5047\n","\n","Step Size: -0.0063\n","Old b3: 2.5047\n","New b3: 2.5109\n","\n","Step Size: -0.0059\n","Old b3: 2.5109\n","New b3: 2.5168\n","\n","Step Size: -0.0055\n","Old b3: 2.5168\n","New b3: 2.5224\n","\n","Step Size: -0.0052\n","Old b3: 2.5224\n","New b3: 2.5276\n","\n","Step Size: -0.0049\n","Old b3: 2.5276\n","New b3: 2.5325\n","\n","Step Size: -0.0046\n","Old b3: 2.5325\n","New b3: 2.5371\n","\n","Step Size: -0.0043\n","Old b3: 2.5371\n","New b3: 2.5414\n","\n","Step Size: -0.0041\n","Old b3: 2.5414\n","New b3: 2.5455\n","\n","Step Size: -0.0038\n","Old b3: 2.5455\n","New b3: 2.5493\n","\n","Step Size: -0.0036\n","Old b3: 2.5493\n","New b3: 2.5529\n","\n","Step Size: -0.0034\n","Old b3: 2.5529\n","New b3: 2.5563\n","\n","Step Size: -0.0032\n","Old b3: 2.5563\n","New b3: 2.5594\n","\n","Step Size: -0.0030\n","Old b3: 2.5594\n","New b3: 2.5624\n","\n","Step Size: -0.0028\n","Old b3: 2.5624\n","New b3: 2.5652\n","\n","Step Size: -0.0026\n","Old b3: 2.5652\n","New b3: 2.5679\n","\n","Step Size: -0.0025\n","Old b3: 2.5679\n","New b3: 2.5704\n","\n","Step Size: -0.0023\n","Old b3: 2.5704\n","New b3: 2.5727\n","\n","Step Size: -0.0022\n","Old b3: 2.5727\n","New b3: 2.5749\n","\n","Step Size: -0.0021\n","Old b3: 2.5749\n","New b3: 2.5769\n","\n","Step Size: -0.0019\n","Old b3: 2.5769\n","New b3: 2.5789\n","\n","Step Size: -0.0018\n","Old b3: 2.5789\n","New b3: 2.5807\n","\n","Step Size: -0.0017\n","Old b3: 2.5807\n","New b3: 2.5824\n","\n","Step Size: -0.0016\n","Old b3: 2.5824\n","New b3: 2.5840\n","\n","Step Size: -0.0015\n","Old b3: 2.5840\n","New b3: 2.5855\n","\n","Step Size: -0.0014\n","Old b3: 2.5855\n","New b3: 2.5869\n","\n","Step Size: -0.0013\n","Old b3: 2.5869\n","New b3: 2.5883\n","\n","Step Size: -0.0013\n","Old b3: 2.5883\n","New b3: 2.5895\n","\n","Step Size: -0.0012\n","Old b3: 2.5895\n","New b3: 2.5907\n","\n","Step Size: -0.0011\n","Old b3: 2.5907\n","New b3: 2.5918\n","\n","Step Size: -0.0010\n","Old b3: 2.5918\n","New b3: 2.5929\n","\n","Step Size: -0.0010\n","Old b3: 2.5929\n","New b3: 2.5938\n","\n","Step Size: -0.0009\n","Old b3: 2.5938\n","New b3: 2.5948\n","\n","Step Size: -0.0009\n","Old b3: 2.5948\n","New b3: 2.5956\n","\n","Step Size: -0.0008\n","Old b3: 2.5956\n","New b3: 2.5964\n","\n","Step Size: -0.0008\n","Old b3: 2.5964\n","New b3: 2.5972\n","\n","Step Size: -0.0007\n","Old b3: 2.5972\n","New b3: 2.5979\n","\n","Step Size: -0.0007\n","Old b3: 2.5979\n","New b3: 2.5986\n","\n","Step Size: -0.0006\n","Old b3: 2.5986\n","New b3: 2.5992\n","\n","Step Size: -0.0006\n","Old b3: 2.5992\n","New b3: 2.5998\n","\n","Step Size: -0.0006\n","Old b3: 2.5998\n","New b3: 2.6004\n","\n","Step Size: -0.0005\n","Old b3: 2.6004\n","New b3: 2.6009\n","\n","Step Size: -0.0005\n","Old b3: 2.6009\n","New b3: 2.6014\n","\n","Step Size: -0.0005\n","Old b3: 2.6014\n","New b3: 2.6019\n","\n","Step Size: -0.0004\n","Old b3: 2.6019\n","New b3: 2.6023\n","\n","Step Size: -0.0004\n","Old b3: 2.6023\n","New b3: 2.6027\n","\n","Step Size: -0.0004\n","Old b3: 2.6027\n","New b3: 2.6031\n","\n","Step Size: -0.0004\n","Old b3: 2.6031\n","New b3: 2.6035\n","\n","Step Size: -0.0003\n","Old b3: 2.6035\n","New b3: 2.6038\n","\n","Step Size: -0.0003\n","Old b3: 2.6038\n","New b3: 2.6041\n","\n","Step Size: -0.0003\n","Old b3: 2.6041\n","New b3: 2.6044\n","\n","Step Size: -0.0003\n","Old b3: 2.6044\n","New b3: 2.6047\n","\n","Step Size: -0.0003\n","Old b3: 2.6047\n","New b3: 2.6050\n","\n","Step Size: -0.0003\n","Old b3: 2.6050\n","New b3: 2.6052\n","\n","Step Size: -0.0002\n","Old b3: 2.6052\n","New b3: 2.6055\n","\n","Step Size: -0.0002\n","Old b3: 2.6055\n","New b3: 2.6057\n","\n","Step Size: -0.0002\n","Old b3: 2.6057\n","New b3: 2.6059\n","\n","Step Size: -0.0002\n","Old b3: 2.6059\n","New b3: 2.6061\n","\n","Step Size: -0.0002\n","Old b3: 2.6061\n","New b3: 2.6063\n","\n","Step Size: -0.0002\n","Old b3: 2.6063\n","New b3: 2.6065\n","\n","Step Size: -0.0002\n","Old b3: 2.6065\n","New b3: 2.6066\n","\n","Step Size: -0.0002\n","Old b3: 2.6066\n","New b3: 2.6068\n","\n","Step Size: -0.0001\n","Old b3: 2.6068\n","New b3: 2.6069\n","\n","Step Size: -0.0001\n","Old b3: 2.6069\n","New b3: 2.6071\n","\n","Step Size: -0.0001\n","Old b3: 2.6071\n","New b3: 2.6072\n","\n","Step Size: -0.0001\n","Old b3: 2.6072\n","New b3: 2.6073\n","\n","Step Size: -0.0001\n","Old b3: 2.6073\n","New b3: 2.6074\n","\n","Step Size: -0.0001\n","Old b3: 2.6074\n","New b3: 2.6075\n","\n","Step Size: -0.0001\n","Old b3: 2.6075\n","New b3: 2.6076\n","\n","b3 Final: 2.6076\n","gradiente Final: -0.0099\n","Total de Iterações: 120\n"]}]},{"cell_type":"markdown","source":["### Animação da Convergência (Ajuste de $b_3$)\n","\n","O código a seguir gera a visualização do treinamento da **Rede Neural** otimizando apenas o viés b3.\n","\n","A função de atualização (`update`) reconstrói a curva de previsão somando o viés atualizado ao resultado fixo das camadas anteriores:\n","\n","$$\n","\\hat{y} = \\underbrace{(h_1 \\cdot w_3 + h_2 \\cdot w_4)}_{\\text{Camadas Ocultas}} + b_{3(\\text{atual})}\n","$$"],"metadata":{"id":"bY78dUfJsgNd"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":699,"output_embedded_package_id":"173o7HnQMyvTkiU-WZbuaYZSO30Io3Hys"},"collapsed":true,"id":"5cb92a42","executionInfo":{"status":"ok","timestamp":1765416415317,"user_tz":180,"elapsed":15321,"user":{"displayName":"artz","userId":"02564237698216966018"}},"outputId":"a057464b-853d-438e-9ee7-ffeb148de51b"},"source":["fig, ax = plt.subplots(figsize=(6, 6))\n","ax.scatter(x_data, y_data, color='blue', zorder=5, label='Pontos Originais')\n","ax.set_title('Gradiente Descendente: Convergência do Parâmetro b3')\n","ax.set_xlabel('Eixo X')\n","ax.set_ylabel('Eixo Y')\n","ax.legend()\n","ax.grid(True)\n","\n","x_curve_anim = np.linspace(min(x_data) - 0.2, max(x_data) + 0.2, 100)\n","\n","curve, = ax.plot(x_curve_anim, neural_network(x_curve_anim, values_of_b3[0]), color='red', label='Curva da Rede Neural')\n","iteration_text = ax.text(0.02, 0.95, '', transform=ax.transAxes, color='green')\n","\n","def update(frame):\n","    current_b3 = values_of_b3[frame]\n","    y_curve_anim = neural_network(x_curve_anim, current_b3)\n","    curve.set_ydata(y_curve_anim)\n","    iteration_text.set_text(f'Iteração: {frame+1}\\nb3: {current_b3:.4f}')\n","    return curve, iteration_text,\n","\n","ani = FuncAnimation(fig, update, frames=len(values_of_b3), interval=80, blit=True)\n","\n","plt.close(fig)\n","HTML(ani.to_jshtml())"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["### Visualizando a Função de Custo e a Tangente\n","\n","* **Curva Roxa:** Representa o erro total (SSR) para diferentes valores de $b_3$. Nosso objetivo é chegar ao fundo desse vale.\n","* **Linha Laranja (Tangente):** Visualiza a derivada no ponto atual.\n","\n","Equação da reta tangente:\n","\n","$$\n","y = \\text{gradiente} \\cdot (x - b_3) + \\text{SSR}\n","$$\n","\n","**Onde:**\n","* **$y$** : O valor da linha tangente que será desenhado\n","* **$b_3$**: A posição atual no eixo X.\n","* **SSR**: A altura atual no eixo Y (o erro).\n","* **Gradiente**: A inclinação da reta ($m$).\n"],"metadata":{"id":"ekiRaSgBueB_"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":499,"output_embedded_package_id":"14IHMErXkEjuPZnjXtcq0TajUG0ELr9xT"},"id":"new_cell_1","executionInfo":{"status":"ok","timestamp":1765416426057,"user_tz":180,"elapsed":10648,"user":{"displayName":"artz","userId":"02564237698216966018"}},"outputId":"4c74f73c-84a4-48a8-8144-32daaf252598"},"source":["intercept_values_plot = np.linspace(0, 5, 200)\n","\n","sum_of_squared_values_plot = [np.sum((y_data - neural_network(x_data, b3_val))**2) for b3_val in intercept_values_plot]\n","\n","fig, ax = plt.subplots(figsize=(6, 4))\n","ax.plot(intercept_values_plot, sum_of_squared_values_plot, color='purple', linestyle='-', label='Função de Perda (SSR)')\n","ax.set_title('Convergência na Função de Perda com Tangente')\n","ax.set_xlabel('Intercept (b3)')\n","ax.set_ylabel('Soma dos Quadrados dos Resíduos (SSR)')\n","ax.legend()\n","ax.grid(True)\n","\n","point, = ax.plot([], [], 'o', color='red', markersize=8, label='Intercepto Atual')\n","tangent_line, = ax.plot([], [], '--', color='orange', label='Linha Tangente')\n","iteration_text = ax.text(0.02, 0.95, '', transform=ax.transAxes, color='green', fontsize=12)\n","\n","def update(frame):\n","    if frame >= len(values_of_b3) or frame >= len(list_sum_of_squared_residuals) or frame >= len(list_current_gradient):\n","        return point, tangent_line, iteration_text\n","\n","    current_intercept = values_of_b3[frame]\n","    current_ssr = list_sum_of_squared_residuals[frame]\n","    current_gradient = list_current_gradient[frame]\n","\n","    point.set_data([current_intercept], [current_ssr])\n","\n","    tangent_x = np.linspace(current_intercept - 0.2, current_intercept + 0.2, 2)\n","    tangent_y = current_gradient * (tangent_x - current_intercept) + current_ssr\n","\n","    tangent_line.set_data(tangent_x, tangent_y)\n","\n","    iteration_text.set_text(f'Iteração: {frame+1}\\nIntercepto: {current_intercept:.4f}\\nSSR: {current_ssr:.4f}')\n","\n","    return point, tangent_line, iteration_text\n","\n","ani = FuncAnimation(fig, update, frames=len(values_of_b3), interval=80, blit=True)\n","\n","plt.close(fig)\n","HTML(ani.to_jshtml())\n"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}